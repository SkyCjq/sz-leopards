---
title: TypeScript Integration Guide
sidebarTitle: Guide
---

<a href="https://github.com/langwatch/langwatch/tree/main/typescript-sdk" target="_blank">
  <img src="https://img.shields.io/badge/repo-langwatch-blue?style=flat-square&logo=Github" noZoom alt="LangWatch TypeScript Repo" onClick="return false;" />
</a>

LangWatch library is the easiest way to integrate your TypeScript application with LangWatch, the messages are synced on the background so it doesn't intercept or block your LLM calls.

import Prerequisites from "/snippets/prerequests-ts.mdx";

<Prerequisites />


## Capturing Messages

- Each message triggering your LLM pipeline as a whole is captured with a [Trace](/concepts#traces).
- A [Trace](/concepts#traces) contains multiple [Spans](/concepts#spans), which are the steps inside your pipeline.
  - This can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
  - Different types of [Spans](/concepts#spans) capture different parameters.
  - [Spans](/concepts#spans) can be nested to capture the pipeline structure.
- [Traces](/concepts#traces) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](/concepts#threads) in their metadata, making the individual messages become part of a conversation.
  - It is also recommended to provide the [`user_id`](/concepts#user-id) metadata to track user analytics.


## Create a Trace

```typescript
// Trace creation
const trace = langwatch.getTrace({
  metadata: { threadId: "mythread-123", userId: "myuser-123" },
});

// Example of updating the metadata to add labels to the trace if needed
trace.update({
  metadata: { labels: ["customer-support"] },
});
```

## Capture an LLM Span

To capture your LLM calls, you can start an LLM span inside the trace with the input about to be sent to the LLM. First, extract the inputs you are going to use for your LLM:

<Tabs>
<Tab title="OpenAI">
```typescript
import { OpenAI } from "openai";

// Model to be used and messages that will be sent to the LLM
const model = "gpt-4o"
const messages : OpenAI.Chat.ChatCompletionMessageParam[] = [
  { role: "system", content: "You are a helpful assistant." },
  {
    role: "user",
    content: "Write a tweet-size vegetarian lasagna recipe for 4 people.",
  },
]
```

Then, start the LLM span from the trace, giving it the model and input messages:

```typescript
const span = trace.startLLMSpan({
  name: "llm",
  model: model,
  input: {
    type: "chat_messages",
    value: messages
  },
});
```

This will capture the LLM input and register the time the call started. Now, continue with the LLM call normally, using the same parameters:

```typescript
const openai = new OpenAI();
const chatCompletion = await openai.chat.completions.create({
  messages: messages,
  model: model,
});
```

Finally, after the OpenAI call is done, end the span to capture the output and register the finish timestamp:

```typescript
span.end({
  outputs: chatCompletion.choices.map((choice) => ({
    type: "chat_messages",
    value: [choice.message],
  })),
});
```
</Tab>
<Tab title="Vercel AI SDK">
```typescript
import { openai } from "@ai-sdk/openai";
import { generateText, type CoreMessage } from "ai";

// Model to be used and messages that will be sent to the LLM
const model = openai("gpt-4o");
const messages: CoreMessage[] = [
  { role: "system", content: "You are a helpful assistant." },
  {
    role: "user",
    content: "Write a tweet-size vegetarian lasagna recipe for 4 people.",
  },
];
```

Then, start the LLM span from the trace, giving it the model and input messages, use the `convertFromVercelAIMessages` utility to convert the input messages from Vercel AI SDK to the format expected by LangWatch:

```typescript
import { convertFromVercelAIMessages } from "langwatch";

const span = trace.startLLMSpan({
  name: "llm",
  model: model.modelId,
  input: {
    type: "chat_messages",
    value: convertFromVercelAIMessages(messages),
  },
});
```

This will capture the LLM input and register the time the call started. Now, continue with the LLM call normally, using the same parameters:

```typescript
const response = await generateText({
  model: model,
  messages: messages,
});
```

Finally, after the Vercel AI SDK call is done, end the span to capture the output and register the finish timestamp:

```typescript
span.end({
  outputs: [
    {
      type: "chat_messages",
      value: convertFromVercelAIMessages(response.responseMessages),
    },
  ],
});
```
</Tab>
</Tabs>

The full trace with all ended spans will be sent automatically to LangWatch in the background on periodic intervals. After capturing your first LLM Span, go to [LangWatch Dashboard](https://app.langwatch.com), your message should be there!

<Note>
On short-live environments like Lambdas or Serverless Functions, be sure to call <br /> `await trace.sendSpans();` to wait for all pending requests to be sent before the runtime is destroyed.
</Note>

## Capture a RAG Span

Appart from LLM spans, another very used type of span is the RAG span. This is used to capture the retrieved contexts from a RAG that will be used by the LLM, and enables a whole new set of RAG-based features evaluations for RAG quality on LangWatch.

import TypeScriptRAG from "/snippets/typescript-rag.mdx";

<TypeScriptRAG />

## Capture an arbritary Span

You can also use generic spans to capture any type of operation, its inputs and outputs, for example for a function call:

```typescript
const span = trace.startSpan({
  name: "weather_function",
  input: {
    type: "json",
    value: {
      city: "Tokyo",
    },
  },
});

weather_function("Tokyo")

span.end({
  outputs: [
    {
      type: "json",
      value: {
        weather: "sunny",
      },
    },
  ],
});
```

You can also nest spans one inside the other, capturing your pipeline structure, for example:

```typescript
const span = trace.startSpan({
  name: "pipeline",
});

const nestedSpan = span.startSpan({
  name: "nested_pipeline",
});

nestedSpan.end()

span.end()
```

Both LLM and RAG spans can also be nested like any arbritary span.