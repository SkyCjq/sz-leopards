## LangEvals

[LangEvals](https://github.com/langwatch/langevals) is the open-source all-in-one library for LLM testing and evaluating in Python, it can be used in notebooks for exploration, in pytest for writting unit tests or as a server API for live-evaluations and guardrails.
LangEvals is modular, including 20+ evaluators such as Ragas for RAG quality, OpenAI Moderation and Azure Jailbreak detection for safety and many others under the same interface.

LangEvals is the backend that powers [LangWatch](https://github.com/langwatch/langwatch) evaluations.

<CardGroup>

<Card title="Quickstart" icon="stars" href="/documentation/quickstart">
  Start evaluating your LLMs in a few lines of code.
</Card>

<Card
  title="Evaluators"
  icon="ballot-check"
  href="/documentation/evaluators"
>
  Learn how to use our evaluators and how to make yours.
</Card>

<Card
  title="PyTest Integration"
  icon="vial"
  href="/documentation/unit-tests"
>
  Create comprehensive testing with extensive edge-case coverage.
</Card>

<Card
  title="Tutorials"
  icon="chalkboard-user"
  href="/tutorials/how-to"
>
  Learn how to evaluate your AI application from our own use cases.
</Card>

</CardGroup>